{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b7e71ec-3ece-4411-bb7a-3d27de410b4d",
   "metadata": {},
   "source": [
    "# Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ce2b6-4f0c-4b81-95f0-74886aeca0ae",
   "metadata": {},
   "source": [
    "## In the context of artificial neural networks, an activation function is a mathematical function that determines the output of a neuron based on its weighted inputs. Activation functions introduce non-linearity to the network, allowing it to model complex relationships between inputs and outputs. Each neuron in a neural network typically applies an activation function to the weighted sum of its inputs and biases.\n",
    "## Activation functions serve two primary purposes: 1. Introduce Non-linearity: Neural networks with only linear transformations can be collapsed into a single linear transformation. Activation functions introduce non-linearity, enabling the network to learn and approximate complex patterns in data.\n",
    "## 2. Enable Learning and Gradient Flow: Activation functions help in adjusting the weights of the network during the training process. They also influence the backpropagation of gradients, allowing efficient learning through optimization algorithms like gradient descent.\n",
    "\n",
    "## There are several types of activation functions commonly used in neural networks: 1. Sigmoid: The sigmoid activation function maps inputs to values between 0 and 1. It was historically used in the past but is less common now due to its vanishing gradient problem.\n",
    "## 2. Hyperbolic Tangent (Tanh): Similar to the sigmoid, the tanh function maps inputs to values between -1 and 1. It has a stronger gradient compared to the sigmoid.\n",
    "## 3. Rectified Linear Unit (ReLU): ReLU is the most widely used activation function. It replaces all negative inputs with zero and passes positive inputs unchanged. It helps mitigate the vanishing gradient problem and speeds up training.\n",
    "## 4. Leaky ReLU: Leaky ReLU is a variation of ReLU that allows a small gradient for negative inputs. This helps address the \"dying ReLU\" problem where certain neurons could become inactive during training.\n",
    "## 5. Parametric ReLU (PReLU): PReLU is similar to Leaky ReLU but allows the slope of the negative part to be learned during training.\n",
    "## 6. Exponential Linear Unit (ELU): ELU is another activation function that addresses the vanishing gradient problem by allowing negative inputs to have non-zero gradients.\n",
    "## 7. Scaled Exponential Linear Unit (SELU): SELU is a self-normalizing activation function that aims to preserve the mean and variance of inputs across layers, promoting stable training and better convergence.\n",
    "## 8. Softmax: Used in the output layer of a neural network for multi-class classification problems. It converts the raw scores into probabilities, making it easier to interpret the model's predictions.\n",
    "## Choosing the right activation function depends on the specific problem you're solving, the architecture of your neural network, and empirical experimentation to see which works best for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c46d6-cfda-43bd-9357-2bdc3191d442",
   "metadata": {},
   "source": [
    "# Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d886c-5eeb-4145-a414-a682c7925228",
   "metadata": {},
   "source": [
    "## There are several common types of activation functions used in neural networks. Each activation function has its own characteristics, advantages, and disadvantages. Here are some of the most widely used activation functions:\n",
    "## 1. Sigmoid Activation Function:\n",
    "\n",
    "## 2. Hyperbolic Tangent (Tanh) Activation Function:\n",
    "\n",
    "## 3. Rectified Linear Unit (ReLU) Activation Function:\n",
    "\n",
    "## 4. Leaky ReLU Activation Function:\n",
    "\n",
    "## 5. Parametric ReLU (PReLU) Activation Function:\n",
    "\n",
    "## 6. Exponential Linear Unit (ELU) Activation Function:\n",
    "\n",
    "## 7. Scaled Exponential Linear Unit (SELU) Activation Function:\n",
    "\n",
    "## 8. Softmax Activation Function:\n",
    "\n",
    "## The choice of activation function depends on factors such as the problem type, network architecture, and empirical performance. Experimenting with different activation functions and monitoring their impact on training convergence and model performance is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc34e9-e603-4fb1-9427-18a6ffa28103",
   "metadata": {},
   "source": [
    "# Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe31f7f-e3db-42aa-9fdb-8fb3ce26eb3b",
   "metadata": {},
   "source": [
    "## Activation functions play a critical role in the training process and performance of a neural network. They influence how information flows through the network and impact its ability to learn and generalize from data. Here's how activation functions affect various aspects of neural network training and performance:\n",
    "## 1. Non-Linearity Introductions: Activation functions introduce non-linearity to the network. Without non-linear activation functions, the entire network would behave like a single linear transformation, severely limiting its ability to capture complex relationships in data. Non-linearity allows the network to approximate arbitrary functions, making it capable of handling a wide range of tasks.\n",
    "## 2. Gradients and Backpropagation: Activation functions affect the gradients that flow backward during the backpropagation process, which is essential for updating the network's weights. If an activation function has gradients that are too small (vanishing gradient) or too large (exploding gradient), it can impede or accelerate the training process. Activation functions that mitigate vanishing gradients, like ReLU and its variants, are preferred because they promote more stable and efficient training.\n",
    "## 3. Convergence Speed: Activation functions can influence how quickly a neural network converges during training. Activation functions with larger gradients for positive inputs (e.g., ReLU) can lead to faster convergence in comparison to activation functions with small gradients (e.g., sigmoid, tanh). Faster convergence can reduce the number of training iterations required to achieve a satisfactory model performance.\n",
    "## 4. Overcoming Saturation: Some activation functions, like sigmoid and tanh, saturate (flatten out) for large positive or negative inputs, leading to a problem known as saturation. During saturation, gradients become very small, making weight updates negligible. ReLU and its variants help mitigate saturation for positive inputs, promoting stronger and more consistent gradients during training.\n",
    "## 5. Regularization and Generalization: Activation functions like dropout and batch normalization can act as implicit forms of regularization. Activation functions that prevent neurons from becoming too dependent on each other during training (e.g., dropout) or normalize activations (e.g., batch normalization) can improve the network's generalization to new, unseen data.\n",
    "## 6. Dying Neurons: Some activation functions, like the plain Leaky ReLU, can suffer from \"dying neurons\" problem, where neurons output constant zero values for a large portion of the input space. This can lead to reduced network capacity and hinder learning. Activation functions like Parametric ReLU and Exponential Linear Unit (ELU) address this issue.\n",
    "## 7. Stable Training and Initialization: Activation functions like SELU are designed to maintain mean and variance across layers, leading to more stable training and easing the process of network initialization. This can result in consistent convergence and better performance.\n",
    "## In summary, activation functions have a profound impact on how neural networks learn and generalize. The choice of activation function should be based on the specific problem, architecture, and empirical experimentation. While some activation functions offer faster training and convergence, others address gradient-related issues or promote better regularization and stability. It's important to choose an activation function that aligns with the desired characteristics of your neural network and the nature of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f7d4f-86cb-4306-9244-60c1a4ab49ea",
   "metadata": {},
   "source": [
    "# Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57fead-9e30-48a8-b40f-8211e622421b",
   "metadata": {},
   "source": [
    "## The sigmoid activation function is a type of non-linear function used in artificial neural networks. It transforms the weighted sum of inputs and biases into a range of values between 0 and 1. The formula for the sigmoid function is:\n",
    "\n",
    "## f(x)= 1/(1+e −x)\n",
    "## Here,  x is the input to the sigmoid function.\n",
    "## Working of the Sigmoid Activation Function:\n",
    "## When the input x is positive, the exponential term e −x becomes small, and the denominator (1+e −x) becomes larger, resulting in a value close to 1. This is how the sigmoid function \"activates\" or passes positive inputs. Conversely, when the inpu x is negative, the exponential term e −x becomes large, causing the denominator (1+e −x) to be close to 1. As a result, the sigmoid function outputs a value close to 0 for negative inputs.\n",
    "\n",
    "## Advantages of the Sigmoid Activation Function: 1. Output Range: The sigmoid function maps inputs to a range between 0 and 1, which can be interpreted as a probability-like value. This makes it suitable for binary classification problems where the output represents the probability of belonging to a particular class.\n",
    "## 2. Smooth Output: The sigmoid function produces a smooth and continuous output, which can be advantageous in certain scenarios where the smoothness of the function is desirable.\n",
    "## 3. Historical Usage: The sigmoid function was historically used in earlier neural networks and is still used in some specific cases, especially when dealing with problems where the output needs to be interpreted as a probability.\n",
    "\n",
    "## Disadvantages of the Sigmoid Activation Function: 1. Vanishing Gradient: One of the major drawbacks of the sigmoid function is the vanishing gradient problem. As the absolute value of the input becomes very large (either very positive or very negative), the gradient of the function approaches zero. This causes the gradients to become too small during backpropagation, making it difficult to update weights and slowing down the training process.\n",
    "## 2. Output Saturation: The sigmoid function's output saturates at 0 or 1 for very positive or very negative inputs. This means that neurons in later layers may receive similar inputs, leading to reduced diversity in activations and slower learning.\n",
    "## 3. Not Zero-Centered: The sigmoid function's output is not zero-centered, which can lead to issues when using gradient-based optimization algorithms.\n",
    "## 4. Alternative Activation Functions: Many modern activation functions, like ReLU and its variants, have been introduced to address the limitations of the sigmoid function. ReLU-based functions generally offer faster convergence and avoid the vanishing gradient problem.\n",
    "## In practice, the sigmoid activation function is less commonly used in hidden layers of deep neural networks due to its drawbacks. ReLU and its variants have gained popularity because they mitigate the vanishing gradient problem and promote faster training while offering better convergence properties. However, the sigmoid function can still find some use in the output layer of models for binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856364b7-89ef-4adb-8b8c-a31984304ddc",
   "metadata": {},
   "source": [
    "# Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9687db-192a-4bd1-a703-ae23039372eb",
   "metadata": {},
   "source": [
    "## The Rectified Linear Unit (ReLU) activation function is a non-linear function commonly used in artificial neural networks, especially in deep learning models. It's known for its simplicity and effectiveness in promoting faster convergence during training. The ReLU function applies a simple threshold operation: it outputs the input if it's positive, and zero otherwise.\n",
    "## The formula for the ReLU function is:  f(x)=max(0,x)\n",
    "## Here, \n",
    "## x is the input to the ReLU function.\n",
    "\n",
    "## Working of the ReLU Activation Function: If the input x is positive (x>0), the ReLU function outputs the same value x. If the input x is negative (x≤0), the ReLU function outputs zero.\n",
    "## Differences Between ReLU and Sigmoid:\n",
    "\n",
    "## Output Range:\n",
    "\n",
    "## Sigmoid: The sigmoid function outputs values between 0 and 1, representing probabilities or scaled activations.\n",
    "## ReLU: The ReLU function outputs values between 0 and the input value itself for positive inputs, effectively preserving the magnitudes of positive activations.\n",
    "## Non-Linearity: Both sigmoid and ReLU are non-linear activation functions. However, ReLU introduces a piecewise linear non-linearity with a single threshold, whereas sigmoid introduces a smooth and S-shaped non-linearity.\n",
    "## Vanishing Gradient:\n",
    "## Sigmoid: The sigmoid function suffers from the vanishing gradient problem for large positive and negative inputs, which can slow down training.\n",
    "## ReLU: ReLU mitigates the vanishing gradient problem for positive inputs, as gradients are either 0 or 1. This enables faster training and convergence.\n",
    "## Convergence Speed: Sigmoid: The saturation of sigmoid outputs for large inputs can lead to slower convergence.\n",
    "## ReLU: ReLU's non-saturation property (except for negative inputs) promotes faster convergence by avoiding the vanishing gradient problem and maintaining larger gradients.\n",
    "## Output Diversity: Sigmoid: Sigmoid outputs tend to be in a narrow range, which can result in similar inputs to neurons in later layers, reducing diversity in activations.\n",
    "## ReLU: ReLU can generate a broader range of outputs, leading to more diverse activations and allowing neurons to learn more independently.\n",
    "## Common Usage:  Sigmoid: Historically used in early neural networks, often in output layers for binary classification.\n",
    "## ReLU: Widely used in hidden layers of deep neural networks due to its faster convergence and alleviation of gradient-related issues.\n",
    "## In summary, the ReLU activation function has become a popular choice for deep learning models due to its effectiveness in addressing the vanishing gradient problem and promoting faster convergence. It offers advantages over the sigmoid function, especially in terms of training speed and the diversity of activations. However, it's worth noting that ReLU-based functions have some drawbacks, such as the \"dying ReLU\" problem, which has led to the development of variants like Leaky ReLU and Parametric ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac96b28-6621-40ff-8049-6c562a79b29f",
   "metadata": {},
   "source": [
    "# Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899bb0ae-4f42-4251-81b6-d8c4eec13331",
   "metadata": {},
   "source": [
    "## Using the Rectified Linear Unit (ReLU) activation function over the sigmoid function offers several significant benefits, especially in the context of training deep neural networks:\n",
    "## Mitigation of Vanishing Gradient Problem: The vanishing gradient problem occurs when gradients become too small during backpropagation, making it difficult to update weights and slowing down training. ReLU helps mitigate this problem because its gradients are either 0 (for negative inputs) or 1 (for positive inputs). This means that gradients can flow more effectively through layers, enabling more efficient weight updates.\n",
    "## Faster Convergence: ReLU's non-saturation property allows it to maintain larger gradients compared to the sigmoid function. As a result, neural networks using ReLU tend to converge faster during training. The faster convergence can significantly reduce the number of training iterations needed to achieve a satisfactory level of performance.\n",
    "## Sparse Activation: When the input to a ReLU neuron is negative, the neuron output is zero. This leads to sparse activations, meaning that only a subset of neurons are activated for any given input. Sparse activations can help the network generalize better by promoting the learning of independent features and reducing overfitting.\n",
    "## Improved Network Capacity: The unbounded nature of ReLU allows neurons to handle a wider range of input values without saturating. This enables neural networks to capture more complex patterns and relationships in the data, thus increasing their capacity to learn and represent information.\n",
    "## Efficiency: The ReLU activation function is computationally efficient to compute compared to the sigmoid function, which involves exponentiation. This efficiency becomes particularly advantageous in large-scale models and during inference in real-world applications.\n",
    "## Diverse Activation: ReLU generates a broader range of outputs compared to sigmoid. This diversity in activation values can help prevent the \"narrowing\" effect where the outputs of neurons become similar in subsequent layers. Diverse activations promote more effective feature learning and model expressiveness.\n",
    "## Simpler Derivatives: The derivative of ReLU is simple: it's 0 for negative inputs and 1 for positive inputs. This simplicity leads to straightforward backpropagation computations, making the training process more stable and understandable.\n",
    "## It's important to note that while ReLU offers these benefits, it's not without its own challenges. The \"dying ReLU\" problem is one such challenge, where neurons can become inactive (output zero) for all inputs and stop learning during training. Variants like Leaky ReLU and Parametric ReLU have been introduced to address this issue. In modern deep learning architectures, ReLU-based activation functions, including its variants, are the default choice for hidden layers due to their demonstrated effectiveness in accelerating training and improving the overall performance of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ad32b-eabe-4257-8b15-29708bc92af0",
   "metadata": {},
   "source": [
    "# Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f6249-c7a8-4a99-8184-217ef8bf25a5",
   "metadata": {},
   "source": [
    "## Leaky ReLU is a variant of the Rectified Linear Unit (ReLU) activation function that aims to address one of the limitations of the original ReLU—specifically, the \"dying ReLU\" problem. The \"dying ReLU\" problem occurs when ReLU neurons output zero for all inputs, effectively rendering them inactive and causing them to \"die\" during training. This happens when the weights and biases of a neuron are adjusted in such a way that the neuron always receives negative inputs.\n",
    "## Leaky ReLU introduces a small slope for negative inputs, allowing a small gradient to flow through the neuron even when the input is negative. The formula for leaky ReLU is as follows:\n",
    "## f(x) = x, if  x≥0\n",
    "## f(x) = ax , if x<0\n",
    "\n",
    "## Here, a is a small positive constant, usually a small fraction like 0.01. When x is negative, the function outputs ax with a slope of a, creating a leaky behavior.\n",
    "## Advantages of Leaky ReLU: 1. Mitigation of Dying ReLU Problem: Leaky ReLU addresses the \"dying ReLU\" problem by allowing a small gradient for negative inputs. This ensures that neurons with negative weights continue to learn and update their weights during training, preventing them from becoming inactive.\n",
    "## 2. Sparse Activation: Similar to the original ReLU, leaky ReLU encourages sparse activations by outputting zero for negative inputs. This can help prevent overfitting by promoting the learning of more diverse features.\n",
    "## 3. Efficient Computation: Leaky ReLU is computationally efficient to compute, similar to ReLU, and can be easily integrated into existing neural network architectures.\n",
    "## Limitations of Leaky ReLU:\n",
    "## 1. Choice of Slope a: The slope a for negative inputs needs to be chosen carefully. While small values like 0.01 are commonly used, the optimal choice can vary based on the dataset and problem. Too large a value for  a may lead to an undesired saturation of the network.\n",
    "## 2. Not Self-Regularizing: While leaky ReLU helps mitigate the \"dying ReLU\" problem, it doesn't inherently address other challenges, such as the vanishing gradient problem. It might not perform as effectively as other activation functions specifically designed for self-regularization, like SELU.\n",
    "## 3. Lack of Theoretical Justification: Leaky ReLU doesn't have a strong theoretical foundation like some other activation functions, such as SELU, which has been designed to maintain stable activations across layers.\n",
    "## In practice, leaky ReLU can be a useful alternative to the original ReLU, especially when you encounter the \"dying ReLU\" problem in your neural network. However, for some problems, other activation functions like Parametric ReLU (PReLU) or Exponential Linear Unit (ELU) might provide better results by offering adaptive negative slopes or addressing the vanishing gradient problem more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c285ba1-e35b-45c2-8ed1-f3608126d99e",
   "metadata": {},
   "source": [
    "# Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d893fd2-fb6b-4438-b430-cd738564debe",
   "metadata": {},
   "source": [
    "## The softmax activation function is used in the output layer of neural networks, particularly in multi-class classification tasks. It converts the raw scores (also known as logits) produced by the network into a probability distribution over multiple classes. The purpose of the softmax function is to provide a way to interpret the network's output as the likelihood that an input belongs to each class.\n",
    "## Purpose of Softmax: 1. Probability Distribution: Softmax transforms the raw scores into probabilities. Each probability represents the likelihood of the input belonging to a particular class. The probabilities sum up to 1, ensuring a valid probability distribution.\n",
    "## 2. Interpretability: The output of a softmax layer can be easily interpreted as class probabilities. This is particularly useful in multi-class classification tasks, where the model needs to assign an input to one of several possible classes.\n",
    "## 3. Loss Calculation: In conjunction with a suitable loss function like categorical cross-entropy, the softmax function enables the computation of the loss between predicted probabilities and the true labels. This loss is then used to update the network's weights during training.\n",
    "## Common Use Cases: Softmax is commonly used in scenarios where an input needs to be classified into one of multiple classes. Some common applications include:\n",
    "## 1. Image Classification: Assigning an image to one of several possible categories, such as recognizing objects in images.\n",
    "## 2. Natural Language Processing: Assigning text documents or sentences to predefined categories, such as sentiment analysis or topic classification.\n",
    "## 3. Speech Recognition: Classifying spoken words or phrases into predefined linguistic categories.\n",
    "## 4. Any Multi-Class Classification: Whenever the output space consists of more than two classes and the goal is to assign an input to one of those classes.\n",
    "## It's important to note that softmax doesn't change the relative order of the raw scores; it simply scales and normalizes them to produce a valid probability distribution. Therefore, it's useful when the goal is to interpret the output as class probabilities, rather than as continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8285b5-2f12-4f17-9a8f-4ef332bd82a0",
   "metadata": {},
   "source": [
    "# Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf169c-05ad-469b-9c30-4e9c30d16b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The hyperbolic tangent (tanh) activation function is another type of non-linear function commonly used in artificial neural networks. It's similar to the sigmoid function in shape but maps input values to a range between -1 and 1. The formula for the hyperbolic tangent function is:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "f(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "−x\n",
    "\n",
    "e \n",
    "−x\n",
    "  becomes large, and the subtraction in the numerator results in a value closer to -1. This makes the tanh function output close to -1 for negative inputs.\n",
    "Comparison Between Tanh and Sigmoid:\n",
    "\n",
    "Output Range:\n",
    "\n",
    "Sigmoid: The sigmoid function outputs values between 0 and 1, representing probabilities or scaled activations.\n",
    "Tanh: The tanh function outputs values between -1 and 1, covering a wider range of negative and positive values.\n",
    "Centered Around Zero:\n",
    "\n",
    "Sigmoid: The sigmoid function is not centered around zero; its outputs are skewed towards one end.\n",
    "Tanh: The tanh function is centered around zero, with negative inputs leading to negative outputs and positive inputs leading to positive outputs.\n",
    "Saturating Outputs:\n",
    "\n",
    "Sigmoid: Both the sigmoid and tanh functions can saturate for very positive or very negative inputs.\n",
    "Tanh: Tanh saturates more aggressively than the sigmoid, as its outputs can be in the range of -1 to 1.\n",
    "Symmetry:\n",
    "\n",
    "Sigmoid: Sigmoid is not symmetric around the origin.\n",
    "Tanh: Tanh is symmetric around the origin, with mirrored shapes for positive and negative inputs.\n",
    "Vanishing Gradient:\n",
    "\n",
    "Both sigmoid and tanh functions can suffer from the vanishing gradient problem for large inputs. However, tanh can have a stronger gradient than sigmoid.\n",
    "Range for Weight Initialization:\n",
    "\n",
    "When using tanh, it's common to initialize weights closer to zero compared to sigmoid, as the output range of tanh is centered around zero.\n",
    "Usage:\n",
    "\n",
    "Tanh was historically used before the widespread adoption of the ReLU family of activation functions. While it can perform better than the sigmoid due to its centered nature, it still suffers from the vanishing gradient problem for large inputs. For this reason, it's often less used in hidden layers of deep neural networks compared to ReLU and its variants.\n",
    "\n",
    "In summary, tanh offers a centered output range that can be useful in specific cases, but it still has limitations related to vanishing gradients. It's generally not the default choice for hidden layers in modern deep learning architectures, where ReLU-based activations are preferred.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
